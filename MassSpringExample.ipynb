{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before reading anything\n",
    "\n",
    "- run the cell below so all the dependencies can precompile as you read the introduction. Plots and DiffEqParamEstim can take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MyModelMenagerie, OrdinaryDiffEq, Plots, FiniteDiff, DiffEqParamEstim;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing a forced harmonic oscillator\n",
    "\n",
    "Here we are going to analyse to death a **really simple** example. One where minimally disruptive (MD) curves can be evolved in fractions of a second. This allows you to play around with all the different features of MinimallyDisruptiveCurves.jl and get immediate feedback.\n",
    "\n",
    "Points that will be covered in this tutorial:\n",
    "\n",
    "- Load prebuilt models from the MyModelMenagerie.jl repository.\n",
    "- Build a differentiable cost function using DiffEqParamEstim.jl.\n",
    "- Evolve a (two-sided) minimally disruptive curve using a cost function and an initial set of parameters.\n",
    "- Sum cost functions (useful if we want to investigate behaviours of a model for multiple inputs, each of which has its own cost function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "A harmonic oscillator is the simplest differential equation model on which we can demonstrate how to find a minimally disruptive curve. The differential equation is provided below:\n",
    "\n",
    "$$ m\\ddot{x}(t) + c \\dot{x}(t) + kx(t) = F(t). $$\n",
    "\n",
    "Physically, this models a mass bouncing up and down on a spring, while being forced by an input $F(t)$. Here $[m,c,k]$ are the mass, damping-coefficient, and spring constant, respectively. $[x(t), \\dot{x}(t), \\ddot{x}(t)]$ are the position, velocity, and acceleration at time $t$.\n",
    "\n",
    "Let's first consider the case where $F(t) = 0$. So we have\n",
    "$$ m\\ddot{x}(t) + c \\dot{x}(t) + kx(t) = 0. $$\n",
    "\n",
    "We can immediately see that the equation doesn't change if we divide through by $m$:\n",
    "\n",
    "$$ \\ddot{x}(t) + \\frac{c}{m} \\dot{x}(t) + \\frac{k}{m}x(t) = 0. $$\n",
    "\n",
    "So for the no-input case, we can see that the model is **invariant** to changes that preserve $\\frac{c}{m}$ and $\\frac{k}{m}$. \n",
    "\n",
    "Let's take $\\theta$ as the vector of parameters $[m,c,k]$, and $\\theta^*$ as a 'nominal' parameter vector. Evolving a minimally disruptive curve equates to generating a parameterised curve $\\theta(s)$ that preserves $\\frac{c}{m} = \\frac{c^*}{m^*}$ and $\\frac{k}{m} = \\frac{k^*}{m^*}$, since any point on this curve represents a parameter set which gives identical model behaviour as $\\theta^*$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "I've made the model for you, in MyModelMenagerie.jl. Take it for a spin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MyModelMenagerie\n",
    "od,ic,tspan, θs = MassSpringOscillator(t->0.);\n",
    "# od: system of ordinary differential equations\n",
    "# ic: initial conditions\n",
    "# tspan: time span to run over\n",
    "# θs: parameters\n",
    "using Latexify\n",
    "latexify(od)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MyModelMenagerie models are built using [ModelingToolkit.jl](https://mtk.sciml.ai).\n",
    "\n",
    "- ```t->0.``` is a function representing the input. You can change to any function of ```t``` that you like.\n",
    "\n",
    "Let's arbitrarily change ic, tspan and ps, just so you can see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tspan = (0., 100.)\n",
    "θs = first.(θs) .=> [2.,1.,4.]\n",
    "ic = first.(ic) .=> [1.,0.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn the model into a differential equation, and solve it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OrdinaryDiffEq, Plots\n",
    "\n",
    "prob = ODEProblem(od,ic,tspan,θs)\n",
    "nom_sol = solve(prob,Tsit5())\n",
    "plot(nom_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make a cost function however you like. MinimallyDisruptiveCurves.jl only cares that the cost function has two methods:\n",
    "```julia\n",
    "function cost(params)\n",
    "   ...\n",
    "    return cost\n",
    "end\n",
    "```\n",
    "\n",
    "```julia\n",
    "function cost(params, gradient_holder)\n",
    "   ...\n",
    "    gradient_holder[:] = ...\n",
    "    # MUTATE gradient_holder so gradient_holder = d(cost)/d(params\n",
    "    return cost\n",
    "end\n",
    "```\n",
    "But we will show you an easy way to do so, using DiffEqParamEstim.jl..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some background. A cost function is a mapping of the form:\n",
    "\n",
    "> parameters -> how badly the model behaves\n",
    "\n",
    "Now you have to choose what your measure of *badly behaved* constitutes. We are just going to take:\n",
    "\n",
    "> parameters -> model -> sol -> |sol - nom_sol|^2 on a set of timepoints\n",
    "\n",
    "So we are using `nom_sol` like experimental data, and taking our cost as the squared deviation from `nom_sol`.\n",
    "Of course, you could use real data, or corrupt the output of `nom_sol` to make things more realistic.\n",
    "\n",
    "How do we build the function? Well let's unroll the cost function:\n",
    "\n",
    "> parameters -> simulate model -> solution -> cost_function(solution, nom_sol)\n",
    "\n",
    "DiffEqParamEstim.jl has an excellent and flexible interface for creating cost functions. You give it a function of the form:\n",
    "> solution -> cost_function(solution, ...)\n",
    "\n",
    "and it will turn this into a differentiable cost function of the parameters, with the two methods specified in the previous paragraph.\n",
    "\n",
    "You can also use DiffEqFlux.jl or DiffEqSensitivity.jl to build your own differentiable cost functions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DiffEqParamEstim;\n",
    "tsteps = tspan[1]:1.:tspan[end]\n",
    "lossf(sol) = sum( [sum(abs2, el1 - el2) for (el1, el2) in zip(sol(tsteps).u, nom_sol(tsteps).u) ] )\n",
    "\n",
    "# or instead use the equivalent convenience function, optimised for speed: \n",
    "lf = L2Loss(tsteps, nom_sol(tsteps).u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = build_loss_objective(prob, Tsit5(), lossf; mpg_autodiff=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! ```cost``` is now a function with two methods. One evaluates the cost as a function of parameters. The other mutates an array with the same shape as the cost gradient. See for yourself:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ₀ = last.(θs)\n",
    "grad_template = deepcopy(θ₀);\n",
    "@show cost(θ₀,grad_template)\n",
    "@show grad_template;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whoops!** \n",
    "- `cost(θ₀)` is a minimum, so we should have `grad_template = [0,0,0]`\n",
    "- Numerical error will always massively corrupt the gradient (in relative terms) near a minimum, where the true gradient is very small.\n",
    "- Don't worry, MinimallyDisruptiveCurves.jl has tricks up it's sleeve to deal with this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial direction for the MD curve\n",
    "\n",
    "- We'll consider this more fully in different examples. For now, we'll be lazy and use the least minimally disruptive direction, to infinitesimal order, as our starting direction.\n",
    "- How? For a small perturbation:\n",
    "$$ C[\\theta + \\delta \\theta] = C[\\theta] + \\delta \\theta^T\\nabla_{\\theta}C[\\theta] + \\frac{1}{2} \\delta \\theta^T\\nabla^2_{\\theta}C[\\theta] \\delta \\theta, $$\n",
    "from Taylor's theorem.\n",
    "- $\\nabla_{\\theta} C[\\theta] = 0$ at the minimum, which gets rid of one term.\n",
    "- $\\nabla^2_{\\theta}C[\\theta] \\succeq 0$ (is positive semidefinite).\n",
    "So we just need to find a $\\delta \\theta$ corresponding to the smallest eigenvalue of $\\nabla^2 C[\\theta]$. \n",
    "- In this case, we will be lazy and just take a finite-difference approximation of the Hessian.\n",
    "- You could replace FiniteDiff with ForwardDiff and run exactly the same code. It will be more accurate. But let's deliberately be a bit inaccurate here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using FiniteDiff\n",
    "#using ForwardDiff\n",
    "H₀ = FiniteDiff.finite_difference_hessian(cost,θ₀)\n",
    "# H₀ = ForwardDiff.hessian(cost, θ₀)\n",
    "using LinearAlgebra\n",
    "@show δθ₀ = eigen(H₀).vectors[:,1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolving a minimally disruptive curve\n",
    "\n",
    "**NB: up to here is independent of MinimallyDisruptiveCurves.jl (but useful didactically (?) ). All you need to bring is a cost function, and an initial direction for the curve, as just described.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's set the momentum. This is **the one hyperparameter** of the algorithm.\n",
    "- Fortunately, the algorithm is (as far as I've seen) pretty robust to the choice of momentum.\n",
    "- Once $ C(\\theta) \\geq mom $, the curve stops.\n",
    "- A larger value of mom (marginally) speeds up curve evolution, and makes the trajectory (marginally) less curvy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom = 1. \n",
    "span = (0., 100.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we also set the desired curve length: `span`.\n",
    "\n",
    "- If span[1] < 0, then two curves are evolved in parallel, pointing in the positive and negative directions of the initial direction, with lengths `abs(span[1]` and `abs(span[2])`.\n",
    "\n",
    "\n",
    "Great! Now we have all the tools to build a curve! First we build a `curveProblem`. This is a struct that contains all the information necessary to generate a curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MinimallyDisruptiveCurves\n",
    "eprob = MDCProblem(cost, θ₀, δθ₀, mom, span);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add some solving options. **For no reason in particular**, let's:\n",
    "- Constrain parameters 1 and 3 to lie in the interval $[0, 1000]$\n",
    "- Ask the solver to provide online output of distance as the curve evolves, on the range 0.1:1:10, and the numerical residual on the derivative of the momentum wrt to the curve direction (which should be zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [\n",
    "    Verbose([CurveDistance(0.1:1:10), HamiltonianResidual(2.3:4:10)]),\n",
    "    ParameterBounds([1,3], [-10.,-10.], [10.,10.])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.206004 seconds (1.39 M allocations: 180.300 MiB, 26.06% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time mdc = evolve(eprob, Tsit5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$<0.15$ seconds to run (the second time) on my laptop! Note that Julia has to precompile functions the first time round, which takes up some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MD curve analysis\n",
    "\n",
    "In different tutorials we will put more emphasis on post-curve analysis (both conceptually and by demonstrating helper functions in the codebase)\n",
    "\n",
    "For now, let's just plot the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = [cost(el) for el in eachcol(trajectory(mdc))];\n",
    "p1= plot(mdc; idxs=[1,2,3], pnames = first.(θs), what = :trajectory);\n",
    "p2 = plot(distances(mdc), log.(cc), ylabel = \"log(cost)\", xlabel = \"distance\", title = \"cost over MD curve\");\n",
    "p3 = plot(mdc; idxs=[1,2,3], pnames = first.(θs), what = :final_changes);\n",
    "p = plot(p1,p2,p3, layout = (3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cost stays at numerical zero ($\\leq exp(-18)$). As we predicted, the ratios $\\frac{c}{m}$ and $\\frac{k}{m}$ are preserved over the curve. \n",
    "\n",
    "Notice the numerical accuracy isn't shabby, given the degree of numerical error in calculations of the cost gradient!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = θ -> [θ[2]/θ[1] ,θ[3]/θ[1]]\n",
    "@show ratios(θ₀)\n",
    "@show ratios(mdc(100)[:states]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing different costs\n",
    "\n",
    "Recall from the preliminary maths that the above minimally disruptive curve only holds in the unforced case, when $F(t) = 0$.\n",
    "\n",
    "Let's now try to find a minimally disruptive curve that holds under **two** conditions:\n",
    "- the unforced case\n",
    "- a sinusoidal forcing term\n",
    "So points on the minimally disruptive curve will correspond to parameters that minimally disrupt behaviour of the nominal system in both of these cases.\n",
    "\n",
    "What do you imagine will happen? For nonzero $F(t)$ it looks clear that all parameters in\n",
    "\n",
    "$$ m\\ddot{x}(t) + c \\dot{x}(t) + kx(t) = F(t). $$\n",
    "will affect behaviour. One regime where the story might change a bit is when $m$ is large. Then:\n",
    "\n",
    "\\begin{align}\n",
    "\\ddot{x}(t) + \\frac{c}{m}\\dot{x}(t) + \\frac{k}{m}x(t) &= \\frac{F(t)}{m} \\\\\n",
    "&\\approx 0.\n",
    "\\end{align}\n",
    "\n",
    "So if $m$ gets big, maybe the minimally disruptive curve will preserve $\\frac{c}{m}$ and $\\frac{k}{m}$? We will find out! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od2,_ , _ , _ = MassSpringOscillator(sin);\n",
    "prob2 = ODEProblem(od2,ic,tspan,θs)\n",
    "nom_sol2 = solve(prob2,Tsit5())\n",
    "plot(nom_sol2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossf2(sol) = sum( [sum(abs2, el1 - el2) for (el1, el2) in zip(sol(tsteps).u, nom_sol2(tsteps).u) ] )\n",
    "cost2 = build_loss_objective(prob2, Tsit5(), lossf2; mpg_autodiff=true);\n",
    "summed_cost = sum_losses([cost, cost2], θ₀);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that above we summed the two cost functions, using `sum_losses`. This works for **any** compatible cost functions. Doesn't matter how they are generated individually.\n",
    "\n",
    "- If you restart julia with multiple threads, than evaluating each subcomponent of the summed cost runs on a separate thread, increasing speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "H₁ = FiniteDiff.finite_difference_hessian(summed_cost,θ₀)\n",
    "δθ₁ = eigen(H₁).vectors[:,1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will change the momentum to 20., so that the curve doesn't terminate early (which it will if cost > mom.).\n",
    "- We will change span so that span[1] < 0. This makes `evolve()` run an MDC curve in two initial directions (on separate threads if they exist). The $\\pm$ directions of $d\\theta_0$, to be precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom = 20.\n",
    "span = (-10.,10.)\n",
    "eprob2 = MDCProblem(summed_cost, θ₀, δθ₁, mom, span);\n",
    "cb = [      Verbose([CurveDistance(0.1:1:10)]),\n",
    "            ParameterBounds([1,2,3], [0.,0.,0.], [1000.,1000.,1000.]),\n",
    "            MomentumReadjustment(NaN)\n",
    "        ]\n",
    "@time mdc2 = evolve(eprob2, Tsit5; mdc_callback=cb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc2 = [summed_cost(el) for el in eachcol(trajectory(mdc2))];\n",
    "p1= plot(mdc2; idxs=[1,2,3], pnames = first.(θs), what = :trajectory);\n",
    "p2 = plot(distances(mdc2), log.(cc2), ylabel = \"log(cost)\", xlabel = \"distance\", title = \"cost over MD curve\");\n",
    "p3 = plot(mdc2; idxs=[1,2,3], pnames = first.(θs), what = :final_changes);\n",
    "p = plot(p1,p2,p3, layout = (3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In **both** directions, the MDcurve eventually found a path that increased mass. Once mass is high, $F(t)$ is comparatively small.\n",
    "- So basically, it couldn't change the parameters in a manner that truly kept the cost small. So it instead made the mass of the pendulum so large that $F(t)$ was insignificant. We can plot this to double check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show cost(mdc2(-10.)[:states])\n",
    "@show cost2(mdc2(-10.)[:states])\n",
    "@show summed_cost(mdc2(-100.)[:states]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob = remake(prob2, p = mdc2(100.)[:states])\n",
    "ss = solve(test_prob, Tsit5())\n",
    "plot(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
