{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming cost functions using MinimallyDisruptiveCurves.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The premise of MinimallyDisruptiveCurves.jl is to\n",
    "> move as far away as possible from the initial parameters, while keeping model behaviour as similar as possible.\n",
    "\n",
    "(in other words, while keeping a cost function as low as possible).\n",
    "\n",
    "The words *as far away as possible* imply some metric on the space of parameters. A lot of key workflows involve manipulating this metric. For instance, by\n",
    "\n",
    "- fixing/freeing parameters that the minimally disruptive curve can change\n",
    "- looking at relative, rather than absolute changes in parameters\n",
    "- biasing parameters to have a larger/smaller influence on the metric, so that minimally disruptive curves are encouraged (not /) to align with them\n",
    "- something custom.\n",
    "\n",
    "Each of these corresponds to a **transformation** of parameter space. The easiest way to do all of these things is by **reparameterising** the cost function $C(\\theta)$. We take a transformation of parameter space: $T(\\theta)$, and a new cost function $D$ satisfying\n",
    "$$ D[T(\\theta)] = C(\\theta). $$\n",
    "\n",
    "**The purpose of this notebook is to show you an easy way to make/perform these reparameterisations.**\n",
    "\n",
    "The (slight) complication is that cost functions compatible with MinimallyDisruptiveCurves.jl must be **differentiable**. That is, they have two methods:\n",
    "```\n",
    "## method 1\n",
    "function cost(ùúÉ)\n",
    "    ...\n",
    "    return cost\n",
    "end\n",
    "\n",
    "## method 2\n",
    "function cost(ùúÉ, grad_template)\n",
    "    ....\n",
    "    grad_template[:] = ‚àáC    # mutate to get gradient wrt parameters\n",
    "    \n",
    "    return cost\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "So we want an easy way of applying composable transformations to cost functions, which also recompute the gradient. Our solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
